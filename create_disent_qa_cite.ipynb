{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code to create the two DisentQA_cite datasets (DisentQA_DupliCite and DisentQA_ParaCite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DisentQA\n",
    "# paper\n",
    "# https://aclanthology.org/2023.acl-long.559.pdf\n",
    "# github\n",
    "# https://github.com/ellaneeman/disent_qa?tab=readme-ov-file\n",
    "# download the data (from their github) -- \"more_baselines/v10-simplified_simplified-nq-train_factual_counterfactual_answerabilty_contextual_baseline_{}_split.csv.zip\" and place in the \"datasets\" folder\n",
    "# https://docs.google.com/document/d/1Z4vA7ifMQTk5YBF3BEYCFSnIvXCYaznLP_7VBcXPEeU/edit?pli=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagi/anaconda3/envs/adaptive_qa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import ast\n",
    "from datasets import Dataset \n",
    "\n",
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});') # clean markups\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "disentqa_train_pandas = pd.read_csv('datasets/v10-simplified_simplified-nq-train_factual_counterfactual_answerabilty_contextual_baseline_train_split.csv')\n",
    "disentqa_val_pandas = pd.read_csv('datasets/v10-simplified_simplified-nq-train_factual_counterfactual_answerabilty_contextual_baseline_val_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(disentqa_train_pandas))\n",
    "print(len(disentqa_val_pandas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disentqa_val_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want is:\n",
    "# question - original_context - conflicting_context - original_answer - conflicting_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# their data is messed up -- similar IDs map to different questions / context, so I need to match info based on the question \n",
    "# (which is the same for different contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_counterfactual = []\n",
    "train_factual = []\n",
    "\n",
    "for idx, row in disentqa_train_pandas.iterrows():\n",
    "    temp_dict = {}\n",
    "    question = row['question']\n",
    "    context = row['context']\n",
    "    parametric_answer = row['parametric_answer']\n",
    "    contextual_answer = row['contextual_answer']\n",
    "    type = row['type']\n",
    "        \n",
    "    if type == 'counterfactual':\n",
    "        temp_dict['question'] = question\n",
    "        temp_dict['conflicting_context'] = cleanhtml(context)\n",
    "        temp_dict['conflicting_answer'] = contextual_answer\n",
    "        temp_dict['original_answer'] = parametric_answer\n",
    "        train_counterfactual.append(temp_dict)\n",
    "    if type == 'factual':\n",
    "        temp_dict['question'] = question\n",
    "        temp_dict['original_context'] = cleanhtml(context)\n",
    "        train_factual.append(temp_dict)\n",
    "\n",
    "# validation\n",
    "val_counterfactual = []\n",
    "val_factual = []\n",
    "for idx, row in disentqa_val_pandas.iterrows():\n",
    "    temp_dict = {}\n",
    "    question = row['question']\n",
    "    context = row['context']\n",
    "    parametric_answer = row['parametric_answer']\n",
    "    contextual_answer = row['contextual_answer']\n",
    "    type = row['type']\n",
    "        \n",
    "    if type == 'counterfactual':\n",
    "        temp_dict['question'] = question\n",
    "        temp_dict['conflicting_context'] = cleanhtml(context)\n",
    "        temp_dict['conflicting_answer'] = contextual_answer\n",
    "        temp_dict['original_answer'] = parametric_answer\n",
    "        val_counterfactual.append(temp_dict)\n",
    "    if type == 'factual':\n",
    "        temp_dict['question'] = question\n",
    "        temp_dict['original_context'] = cleanhtml(context)\n",
    "        val_factual.append(temp_dict)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_counterfactual_df = pd.DataFrame(train_counterfactual)\n",
    "train_factual_df = pd.DataFrame(train_factual)\n",
    "\n",
    "# validation\n",
    "val_counterfactual_df = pd.DataFrame(val_counterfactual)\n",
    "val_factual_df = pd.DataFrame(val_factual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes based on the question string\n",
    "# This will perform and \"inner-join\" thereby omitting rows in each dataframe that do not match. \n",
    "# Hence, no NaN in either the right or left part of merged dataframe.\n",
    "\n",
    "# train\n",
    "train_merged_df = train_factual_df.merge(train_counterfactual_df, how = 'inner', on = ['question'])\n",
    "\n",
    "# validation\n",
    "val_merged_df = val_factual_df.merge(val_counterfactual_df, how = 'inner', on = ['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_merged_df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_merged_df))\n",
    "print(len(val_merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_contexts = list(train_merged_df[\"original_context\"])\n",
    "conflicting_context = list(train_merged_df[\"conflicting_context\"])\n",
    "train_cited_context = [f'Document 1: {original_contexts[i]}. Document 2: {conflicting_context[i]}' for i in range(len(train_merged_df))]\n",
    "train_cited_context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_answer = list(train_merged_df[\"original_answer\"])\n",
    "conflicting_answer = list(train_merged_df[\"conflicting_answer\"])\n",
    "train_cited_answer = [f'According to Document 1 the answer is: {original_answer[i]}. According to Document 2 the answer is: {conflicting_answer[i]}' for i in range(len(train_merged_df))]\n",
    "train_cited_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_contexts = list(val_merged_df[\"original_context\"])\n",
    "conflicting_context = list(val_merged_df[\"conflicting_context\"])\n",
    "val_cited_context = [f'Document 1: {original_contexts[i]}. Document 2: {conflicting_context[i]}' for i in range(len(val_merged_df))]\n",
    "val_cited_context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_answer = list(val_merged_df[\"original_answer\"])\n",
    "conflicting_answer = list(val_merged_df[\"conflicting_answer\"])\n",
    "val_cited_answer = [f'According to Document 1 the answer is: {original_answer[i]}. According to Document 2 the answer is: {conflicting_answer[i]}' for i in range(len(val_merged_df))]\n",
    "val_cited_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_merged_df['cited_context'] = val_cited_context\n",
    "val_merged_df['cited_answer'] = val_cited_answer\n",
    "val_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_df['cited_context'] = train_cited_context\n",
    "train_merged_df['cited_answer'] = train_cited_answer\n",
    "train_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(val_merged_df))\n",
    "print(len(train_merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df if the context is too long (which will cause an error / the model wont generate anything)\n",
    "# max_length = 3000\n",
    "# val_merged_df.loc[len(val_merged_df['cited_context'].split(' ')) < max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionaries\n",
    "train_dict = train_merged_df.to_dict('list') \n",
    "# save final dataset to json\n",
    "# with open('datasets/disent_qa_train.json', 'w') as fp:\n",
    "#     json.dump(train_dict, fp)    \n",
    "\n",
    "# split val into val and test\n",
    "val_merged_df_1 = val_merged_df[:len(val_merged_df)//2]\n",
    "val_merged_df_2 = val_merged_df[len(val_merged_df)//2:]\n",
    "\n",
    "val_dict = val_merged_df_1.to_dict('list')\n",
    "test_dict = val_merged_df_2.to_dict('list')\n",
    "\n",
    "# with open('datasets/disent_qa_val.json', 'w') as fp:\n",
    "#     json.dump(val_dict, fp)\n",
    "\n",
    "# with open('datasets/disent_qa_test.json', 'w') as fp:\n",
    "#     json.dump(test_dict, fp)\n",
    "\n",
    "# load\n",
    "# with open('datasets/disent_qa_train.json', 'r') as fp:\n",
    "#     disent_qa_train = json.load(fp)\n",
    "# with open('datasets/disent_qa_dev.json', 'r') as fp:\n",
    "#     disent_qa_dev = json.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open('datasets/disent_qa_val.json', 'r') as fp:\n",
    "    disent_qa_val = json.load(fp)\n",
    "with open('datasets/disent_qa_val.json', 'r') as fp:\n",
    "    disent_qa_val = json.load(fp)    \n",
    "with open('datasets/disent_qa_test.json', 'r') as fp:\n",
    "    disent_qa_test = json.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629.8752922837101"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [f'{cited_context}' for cited_context in\\\n",
    "         zip(disent_qa_test['cited_context'])]\n",
    "\n",
    "words_per_prompt = [len(i.split(' ')) for i in prompts]\n",
    "avg_word_count = sum(words_per_prompt)/len(words_per_prompt)\n",
    "avg_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "who gave the first in person state of the union\n",
      "original_context\n",
      " The address fulfills rules in Article II , Section 3 of the U.S. Constitution , requiring the President to periodically `` give to the Congress Information of the State of the Union , and recommend to their Consideration such measures as he shall judge necessary and expedient . '' During most of the country 's first century , the President primarily only submitted a written report to Congress . After 1913 , Woodrow Wilson , the 28th U.S. President , began the regular practice of delivering the address to Congress in person as a way to rally support for his agenda . With the advent of radio and television , the address is now broadcast live across the country on many networks . \n",
      "conflicting_context\n",
      " The address fulfills rules in Article II , Section 3 of the U.S. Constitution , requiring the President to periodically `` give to the Congress Information of the State of the Union , and recommend to their Consideration such measures as he shall judge necessary and expedient . '' During most of the country 's first century , the President primarily only submitted a written report to Congress . After 1913 , Angela Hunte , the 28th U.S. President , began the regular practice of delivering the address to Congress in person as a way to rally support for his agenda . With the advent of radio and television , the address is now broadcast live across the country on many networks . \n",
      "conflicting_answer\n",
      "Angela Hunte\n",
      "original_answer\n",
      "Woodrow Wilson\n",
      "cited_context\n",
      "Document 1:  The address fulfills rules in Article II , Section 3 of the U.S. Constitution , requiring the President to periodically `` give to the Congress Information of the State of the Union , and recommend to their Consideration such measures as he shall judge necessary and expedient . '' During most of the country 's first century , the President primarily only submitted a written report to Congress . After 1913 , Woodrow Wilson , the 28th U.S. President , began the regular practice of delivering the address to Congress in person as a way to rally support for his agenda . With the advent of radio and television , the address is now broadcast live across the country on many networks . . Document 2:  The address fulfills rules in Article II , Section 3 of the U.S. Constitution , requiring the President to periodically `` give to the Congress Information of the State of the Union , and recommend to their Consideration such measures as he shall judge necessary and expedient . '' During most of the country 's first century , the President primarily only submitted a written report to Congress . After 1913 , Angela Hunte , the 28th U.S. President , began the regular practice of delivering the address to Congress in person as a way to rally support for his agenda . With the advent of radio and television , the address is now broadcast live across the country on many networks . \n",
      "cited_answer\n",
      "According to Document 1 the answer is: Woodrow Wilson. According to Document 2 the answer is: Angela Hunte\n"
     ]
    }
   ],
   "source": [
    "for k in disent_qa_test.keys():\n",
    "    print(k)\n",
    "    print(disent_qa_test[k][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create paraphrased disetqa (DisentQA_ParaCite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm using ChatGPT to paraphrase the context (the original dataset has context duplications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'original_context', 'conflicting_context', 'conflicting_answer', 'original_answer', 'cited_context', 'cited_answer'],\n",
       "    num_rows: 3849\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disent_qa_test_hf = Dataset.from_dict(disent_qa_test) \n",
    "disent_qa_test_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '' # put API key here\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# single string\n",
    "def chat_gpt(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# for batches\n",
    "# def chat_gpt(list_of_contexts, list_of_answers):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": f\"{list_of_contexts}\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": f\"Paraphrase every element of the array. Ensure that these corresponding answers: {list_of_answers}, are still in the paraphrased output. Reply with an array of all completions.\"\n",
    "#         }\n",
    "#     ]\n",
    "#     )\n",
    "#     print(f\"{list_of_contexts}\")\n",
    "#     response_string = response.choices[0].message.content.strip()\n",
    "#     print(response_string)\n",
    "#     response_list = ast.literal_eval(response_string)\n",
    "#     return response_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if starting from scratch\n",
    "# paraphrased_context = [] \n",
    "\n",
    "# while len(paraphrased_context) < len(disent_qa_test_hf): # until we paraphrase all text (in case it gives an error)\n",
    "#     for entry_idx in range(len(paraphrased_context), len(disent_qa_test_hf)):\n",
    "#         try:\n",
    "#             if entry_idx % 20 == 0:\n",
    "#                 print(f'Counter: {entry_idx} out of {len(disent_qa_test_hf)}')\n",
    "#                 # save summarized dataset\n",
    "#                 with open('paraphrased_disentqa.json', 'w') as fp:\n",
    "#                     json.dump(paraphrased_context, fp)     \n",
    "\n",
    "#             # extract\n",
    "#             conflicting_context = disent_qa_test_hf[entry_idx]['conflicting_context'] # text to be paraphrased\n",
    "#             conflicting_label = disent_qa_test_hf[entry_idx]['conflicting_answer'] \n",
    "\n",
    "#             # Give the text to the model and ask for a summary using the GPT-3.5-turbo model (cost money)\n",
    "            \n",
    "#             paraphrase = chat_gpt(f\"Paraphrase this: {conflicting_context}. Ensure that '{conflicting_label}' is still in the paraphrased output\")\n",
    "\n",
    "#             paraphrased_context.append(paraphrase)\n",
    "#         except: # 'RateLimitError' -- cant use it as an exception so leaving this blank\n",
    "#             print('sleeping...')\n",
    "#             time.sleep(10) # Delay for 1 minute (60 seconds).\n",
    "        \n",
    "# # Make sure to save the final batch of paraphrased data\n",
    "# with open('paraphrased_disentqa.json', 'w') as fp:\n",
    "#     json.dump(paraphrased_context, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('paraphrased_disentqa.json', 'r') as fp:\n",
    "#     paraphrased_disentqa = json.load(fp)    \n",
    "# print(len(paraphrased_disentqa))\n",
    "# print(len(disent_qa_test_hf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the lists are the same -- they are not in 2 places because the API broke\n",
    "# idx = 660\n",
    "# print(paraphrased_disentqa[idx])\n",
    "# print()\n",
    "# print(disent_qa_test_hf[idx]['conflicting_context'])\n",
    "# print()\n",
    "# print(disent_qa_test_hf[idx]['conflicting_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_paraphrase = paraphrased_disentqa.copy()\n",
    "# new_paraphrase.insert(661, 'NA')\n",
    "# new_paraphrase.insert(769, 'NA')\n",
    "\n",
    "# new_paraphrase = new_paraphrase[:len(disent_qa_test_hf)] # added 2 NA, need to remove the last 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 765\n",
    "# for i in range(20):\n",
    "#     new_idx = idx + i\n",
    "#     print('new_idx', new_idx)\n",
    "#     print(disent_qa_test_hf[new_idx]['conflicting_context'])\n",
    "#     print()\n",
    "#     print(new_paraphrase[new_idx])\n",
    "#     print()\n",
    "#     print(disent_qa_test_hf[new_idx]['conflicting_answer'])\n",
    "#     print()\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('datasets/clean_paraphrased_disentqa.json', 'w') as fp:\n",
    "#     json.dump(new_paraphrase, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/clean_paraphrased_disentqa.json', 'r') as fp:\n",
    "    clean_paraphrased_disentqa = json.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_paraphrased_disentqa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disent_qa_test['conflicting_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disent_qa_test['cited_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the conflicting context in disentqa with the new paraphrased conflicting context\n",
    "paraphrased_disentqa_test = disent_qa_test.copy()\n",
    "print(paraphrased_disentqa_test.keys())\n",
    "paraphrased_disentqa_test.pop('conflicting_context', None) # remove old one\n",
    "paraphrased_disentqa_test['conflicting_context'] = clean_paraphrased_disentqa # add new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cited_context = [f'Document 1: {original_context}. Document 2: {paraphrased_context}' for original_context, paraphrased_context in zip(disent_qa_test['original_context'],clean_paraphrased_disentqa)]\n",
    "new_cited_context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_disentqa_test.pop('cited_context', None) # remove old one\n",
    "paraphrased_disentqa_test['cited_context'] = new_cited_context # add new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrased_disentqa_test['cited_context'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/paraphrased_disentqa_test.json', 'w') as fp:\n",
    "    json.dump(paraphrased_disentqa_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(disent_qa_test['conflicting_context'][0])\n",
    "# print(paraphrased_disentqa_test['conflicting_context'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(disent_qa_test['conflicting_context']))\n",
    "# print(len(paraphrased_disentqa_test['conflicting_context']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01f989aa223c0c84311f46d7580eb8fa94f35fe0e1d6e5c1d38b7062bc3d2b8b"
  },
  "kernelspec": {
   "display_name": "Python 3.12.3 ('oracle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
