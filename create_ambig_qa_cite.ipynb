{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code to create AmbigQA_cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AmbigQA (AmbigNQ)\n",
    "# paper\n",
    "# https://arxiv.org/pdf/2004.10645.pdf\n",
    "# github\n",
    "# https://github.com/shmsw25/AmbigQA/?tab=readme-ov-file#dataset-contents\n",
    "# download the data (from their github -- \"AmbigNQ with evidence articles\") and put it in the \"datasets\" folder\n",
    "# https://github.com/shmsw25/AmbigQA/?tab=readme-ov-file#dataset-contents\n",
    "\n",
    "# note that there is also data under \"AmbigNQ\", but the text \"snippet\" is bad (not full sentences, lots of \"...\", and is in markdown (<b>...<b>))\n",
    "# so I'm using the \"evidence articles\" version and will truncate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "with open('datasets/train_with_evidence_articles.json') as f:\n",
    "    ambigqa_train_dict = json.load(f)\n",
    "\n",
    "# val\n",
    "with open('datasets/dev_with_evidence_articles.json') as f: # note that I'll need to split this in 2: val and test\n",
    "    ambigqa_val_dict = json.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "ambigqa_train_pandas = pd.DataFrame.from_dict(ambigqa_train_dict)\n",
    "\n",
    "# val\n",
    "ambigqa_val_pandas = pd.DataFrame.from_dict(ambigqa_val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ambigqa_train_pandas))\n",
    "print(len(ambigqa_val_pandas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambigqa_val_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want is:\n",
    "# question - original_context - conflicting_context - original_answer - conflicting_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_questions = []\n",
    "train_answers = []\n",
    "train_texts = []\n",
    "\n",
    "for idx, row in ambigqa_train_pandas.iterrows():\n",
    "    if any([row['annotations'][i]['type']=='multipleQAs' for i in range(len(row['annotations']))]):\n",
    "        question = row['question']\n",
    "        text = row['articles_plain_text']\n",
    "        annotations = row['annotations']\n",
    "        \n",
    "        tmp_train_answers = []\n",
    "        for entry in annotations:\n",
    "            try:\n",
    "                qaPairs = entry['qaPairs']\n",
    "                for qa_pair in qaPairs:\n",
    "                    unambiguous_question = qa_pair['question'] # since we just want the main, ambiguous question, we can ignore these specific ones\n",
    "                    unambiguous_train_answers_list = qa_pair['answer']\n",
    "                    unambiguous_answer = qa_pair['answer'][0] # just take the first answer for each question\n",
    "                    tmp_train_answers.append(unambiguous_answer)\n",
    "                tmp_train_answers = list(set(tmp_train_answers)) # remove dupilcates\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "        train_questions.append(question)\n",
    "        train_texts.append(text)\n",
    "        train_answers.append(tmp_train_answers)\n",
    "\n",
    "# val\n",
    "val_questions = []\n",
    "val_answers = []\n",
    "val_texts = []\n",
    "\n",
    "for idx, row in ambigqa_val_pandas.iterrows():\n",
    "    if any([row['annotations'][i]['type']=='multipleQAs' for i in range(len(row['annotations']))]):\n",
    "        question = row['question']\n",
    "        text = row['articles_plain_text']\n",
    "        annotations = row['annotations']\n",
    "        \n",
    "        tmp_val_answers = []\n",
    "        for entry in annotations:\n",
    "            try:\n",
    "                qaPairs = entry['qaPairs']\n",
    "                for qa_pair in qaPairs:\n",
    "                    unambiguous_question = qa_pair['question'] # since we just want the main, ambiguous question, we can ignore these specific ones\n",
    "                    unambiguous_val_answers_list = qa_pair['answer']\n",
    "                    unambiguous_answer = qa_pair['answer'][0] # just take the first answer for each question\n",
    "                    tmp_val_answers.append(unambiguous_answer)\n",
    "                tmp_val_answers = list(set(tmp_val_answers)) # remove dupilcates\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "        val_questions.append(question)\n",
    "        val_texts.append(text)\n",
    "        val_answers.append(tmp_val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/77929243/find-unique-answers-from-list-of-strings\n",
    "def find_all_answer_combinations(texts, answers, used_texts=()):\n",
    "    if not texts or not answers:\n",
    "        # Nothing to do, return an empty combination.\n",
    "        yield ()\n",
    "        return\n",
    "    \n",
    "    # Pick an answer to process.\n",
    "    answer, *rest_answers = answers\n",
    "    # Find solutions that *don't* use the selected answer.\n",
    "    yield from find_all_answer_combinations(texts, rest_answers, used_texts)\n",
    "\n",
    "    if any(answer in text for text in used_texts):\n",
    "        # The answer already appears in a returned text, so we can't use it again.\n",
    "        return\n",
    "\n",
    "    # Find texts that have this answer.\n",
    "    matched_texts = [text for text in texts if answer in text]\n",
    "    # Only continue checking texts that don't contain the answer,\n",
    "    # otherwise we'd have two or more matches for this answer.\n",
    "    other_texts = [text for text in texts if answer not in text]\n",
    "    # Try generating combinations with each of the matched texts.\n",
    "    for matched_text in matched_texts:\n",
    "        # Assuming we use `matched_text`, generate combinations\n",
    "        # for the rest of answers/texts.\n",
    "        for result in find_all_answer_combinations(other_texts, rest_answers, used_texts+(matched_text,)):\n",
    "            yield [answer, matched_text], *result\n",
    "\n",
    "def find_best_answers(texts, answers):\n",
    "    # Finds the combination with most answers.\n",
    "    return list(max(find_all_answer_combinations(texts, answers), key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [\"this is string 1 this is string 3\", \"this is string 2\", \"this is string 3\"]\n",
    "# texts = [\"this is string 1\", \"this is string 3\", \"this is string 1 this is string 3\"]\n",
    "# answers = [\"string 1\",\"this is\", \"string 3\"]\n",
    "\n",
    "# find_best_answers(texts, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_final_questions = []\n",
    "train_original_full_context = []\n",
    "train_conflicting_full_context = []\n",
    "train_original_answer = []\n",
    "train_conflicting_answer = []\n",
    "\n",
    "for qst, ans, txt in zip(train_questions, train_answers, train_texts):\n",
    "    output = find_best_answers(txt, ans)\n",
    "    if len(output) == 2: # only use those that have exactly 2 answers and corresponding docs (some have 3 or 4, and I could create duplicates of each combination to get a little bit more data, but I'll avoid it for now)\n",
    "        train_final_questions.append(qst)\n",
    "        train_original_full_context.append(output[0][1]) # first one can be used as the original\n",
    "        train_original_answer.append(output[0][0]) # second one can be used as the conflicting\n",
    "        train_conflicting_full_context.append(output[1][1]) # first one can be used as the original\n",
    "        train_conflicting_answer.append(output[1][0]) # second one can be used as the conflicting\n",
    "        \n",
    "# val\n",
    "val_final_questions = []\n",
    "val_original_full_context = []\n",
    "val_conflicting_full_context = []\n",
    "val_original_answer = []\n",
    "val_conflicting_answer = []\n",
    "\n",
    "for qst, ans, txt in zip(val_questions, val_answers, val_texts):\n",
    "    output = find_best_answers(txt, ans)\n",
    "    if len(output) == 2: # only use those that have exactly 2 answers and corresponding docs (some have 3 or 4, and I could create duplicates of each combination to get a little bit more data, but I'll avoid it for now)\n",
    "        val_final_questions.append(qst)\n",
    "        val_original_full_context.append(output[0][1]) # first one can be used as the original\n",
    "        val_original_answer.append(output[0][0]) # second one can be used as the conflicting\n",
    "        val_conflicting_full_context.append(output[1][1]) # first one can be used as the original\n",
    "        val_conflicting_answer.append(output[1][0]) # second one can be used as the conflicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full context is too long, so let's take a snippet of k characters around the answer. This is like the top-k method, but with characters\n",
    "def extract_substring_with_keywords(text, keyword, k=300):\n",
    "    words = text[max(0,text.index(keyword) - k) : min(len(text),text.index(keyword) + k)].split(' ')\n",
    "    substring = ' '.join(words[1:-1]) # remove the first and last because they might be truncated\n",
    "    return substring\n",
    "\n",
    "\n",
    "# train\n",
    "train_original_contexts = [extract_substring_with_keywords(train_original_full_context_entry, answer_entry) for train_original_full_context_entry, answer_entry in zip(train_original_full_context,train_original_answer)]\n",
    "train_conflicting_contexts = [extract_substring_with_keywords(train_conflicting_full_context_entry, answer_entry) for train_conflicting_full_context_entry, answer_entry in zip(train_conflicting_full_context,train_conflicting_answer)]\n",
    "\n",
    "# val\n",
    "val_original_contexts = [extract_substring_with_keywords(val_original_full_context_entry, answer_entry) for val_original_full_context_entry, answer_entry in zip(val_original_full_context,val_original_answer)]\n",
    "val_conflicting_contexts = [extract_substring_with_keywords(val_conflicting_full_context_entry, answer_entry) for val_conflicting_full_context_entry, answer_entry in zip(val_conflicting_full_context,val_conflicting_answer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_df = pd.DataFrame(\n",
    "    {'question': train_final_questions,\n",
    "     'original_context': train_original_contexts,\n",
    "     'conflicting_context': train_conflicting_contexts,\n",
    "     'original_answer': train_original_answer,\n",
    "     'conflicting_answer': train_conflicting_answer\n",
    "    })\n",
    "\n",
    "# val\n",
    "val_df = pd.DataFrame(\n",
    "    {'question': val_final_questions,\n",
    "     'original_context': val_original_contexts,\n",
    "     'conflicting_context': val_conflicting_contexts,\n",
    "     'original_answer': val_original_answer,\n",
    "     'conflicting_answer': val_conflicting_answer\n",
    "    })\n",
    "\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_contexts = list(train_df[\"original_context\"])\n",
    "conflicting_context = list(train_df[\"conflicting_context\"])\n",
    "train_cited_context = [f'Document 1: {original_contexts[i]}. Document 2: {conflicting_context[i]}' for i in range(len(train_df))]\n",
    "\n",
    "original_answer = list(train_df[\"original_answer\"])\n",
    "conflicting_answer = list(train_df[\"conflicting_answer\"])\n",
    "train_cited_answer = [f'According to Document 1 the answer is: {original_answer[i]}. According to Document 2 the answer is: {conflicting_answer[i]}' for i in range(len(train_df))]\n",
    "\n",
    "train_df['cited_context'] = train_cited_context\n",
    "train_df['cited_answer'] = train_cited_answer\n",
    "\n",
    "original_contexts = list(val_df[\"original_context\"])\n",
    "conflicting_context = list(val_df[\"conflicting_context\"])\n",
    "val_cited_context = [f'Document 1: {original_contexts[i]}. Document 2: {conflicting_context[i]}' for i in range(len(val_df))]\n",
    "\n",
    "original_answer = list(val_df[\"original_answer\"])\n",
    "conflicting_answer = list(val_df[\"conflicting_answer\"])\n",
    "val_cited_answer = [f'According to Document 1 the answer is: {original_answer[i]}. According to Document 2 the answer is: {conflicting_answer[i]}' for i in range(len(val_df))]\n",
    "\n",
    "val_df['cited_context'] = val_cited_context\n",
    "val_df['cited_answer'] = val_cited_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionaries\n",
    "train_dict = train_df.to_dict('list') \n",
    "# save final dataset to json\n",
    "with open('datasets/ambig_qa_train.json', 'w') as fp:\n",
    "    json.dump(train_dict, fp)    \n",
    "\n",
    "# split val into val and test\n",
    "val_df_1 = val_df[:len(val_df)//2]\n",
    "val_df_2 = val_df[len(val_df)//2:]\n",
    "\n",
    "val_dict = val_df_1.to_dict('list')\n",
    "test_dict = val_df_2.to_dict('list')\n",
    "\n",
    "with open('datasets/ambig_qa_val.json', 'w') as fp:\n",
    "    json.dump(val_dict, fp)\n",
    "\n",
    "with open('datasets/ambig_qa_test.json', 'w') as fp:\n",
    "    json.dump(test_dict, fp)\n",
    "\n",
    "# load\n",
    "# with open('datasets/ambig_qa_train.json', 'r') as fp:\n",
    "#     ambig_qa_train = json.load(fp)\n",
    "# with open('datasets/ambig_qa_dev.json', 'r') as fp:\n",
    "#     ambig_qa_dev = json.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "with open('datasets/ambig_qa_train.json', 'r') as fp:\n",
    "    ambig_qa_train = json.load(fp)\n",
    "with open('datasets/ambig_qa_val.json', 'r') as fp:\n",
    "    ambig_qa_val = json.load(fp)\n",
    "with open('datasets/ambig_qa_test.json', 'r') as fp:\n",
    "    ambig_qa_test = json.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [f'{cited_context}' for cited_context in\\\n",
    "         zip(ambig_qa_test['cited_context'])]\n",
    "\n",
    "words_per_prompt = [len(i.split(' ')) for i in prompts]\n",
    "avg_word_count = sum(words_per_prompt)/len(words_per_prompt)\n",
    "avg_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambig_qa_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Who voices rocket raccoon in guardians of the galaxy?\"\n",
    "for entry_idx, entry in enumerate(ambig_qa_test['question']):\n",
    "    if entry == q:\n",
    "        print(q)\n",
    "        print(ambig_qa_test['original_context'][entry_idx])\n",
    "        print(ambig_qa_test['original_answer'][entry_idx])\n",
    "        print()\n",
    "        print(ambig_qa_test['conflicting_context'][entry_idx])\n",
    "        print(ambig_qa_test['conflicting_answer'][entry_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01f989aa223c0c84311f46d7580eb8fa94f35fe0e1d6e5c1d38b7062bc3d2b8b"
  },
  "kernelspec": {
   "display_name": "Python 3.12.3 ('oracle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
