{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code to analyze the results from the different datasets and settings (e.g., different number of in-context examples, distractors, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True # will stop __pycache__ from being generated \n",
    "import json\n",
    "from evaluation_method import overall_evaluation\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'results/zero_shot'\n",
    "# base_path = 'results/few_shot'\n",
    "# base_path = 'results/1_step_cot'\n",
    "# base_path = 'results/1_step_cot_paraphrase'\n",
    "# base_path = 'results/2_step_cot'\n",
    "# base_path = 'results/prompt_chaining'\n",
    "# base_path = 'results/finetune'\n",
    "for file_name in os.listdir(base_path):\n",
    "    # if 'disent_qa' in file_name and 'NO_CONFLICT' in file_name:\n",
    "    # if 'disent_qa' in file_name and 'NO_CONFLICT' not in file_name and '3_shot' in file_name:\n",
    "    # if 'disent_qa' in file_name:\n",
    "    # if 'ambig_qa' in file_name:\n",
    "    # if 'hotpotqa' in file_name:\n",
    "    if 'distractor_hotpotqa' not in file_name:\n",
    "    # if 'paraphrase' in file_name:\n",
    "        print(f'file_name: {file_name}')\n",
    "        path = f'{base_path}/{file_name}'\n",
    "        with open(path, 'r') as fp:\n",
    "            results_dict = json.load(fp)\n",
    "            if 'hotpotqa' not in  file_name:\n",
    "                # cited_answers = [[f'According to Document 1 the answer is: {results_dict[\"original_answer\"][i]}.', f'According to Document 2 the answer is: {results_dict[\"conflicting_answer\"][i]}']\n",
    "                cited_answers = [[f'According to Document 1 the answer is {results_dict[\"original_answer\"][i]}.', f'According to Document 2 the answer is {results_dict[\"conflicting_answer\"][i]}'] # ensure that the answer is still correct even if it's missing the \":\" (i.e., \"is:\" vs \"is\")\n",
    "                            for i in range(len(results_dict['conflicting_answer']))]\n",
    "                scores = overall_evaluation([i.replace('is:','is') for i in results_dict['model_generated_answer']], results_dict['gold_answers'], cited_answers) # split the cited answer to check if any exist in the generated answer\n",
    "            else:\n",
    "                cited_answers = [\n",
    "                    [results_dict[\"cited_original_answers\"][i],\n",
    "                     results_dict[\"cited_conflicting_answers_1\"][i],\n",
    "                     results_dict[\"cited_conflicting_answers_2\"][i],\n",
    "                     ] for i in range(len(results_dict['cited_original_answers']))]\n",
    "                scores = overall_evaluation([i.replace('is:','is') for i in results_dict['model_generated_answer']], results_dict['gold_answers'], cited_answers) # split the cited answer to check if any exist in the generated answer\n",
    "            print(scores)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01f989aa223c0c84311f46d7580eb8fa94f35fe0e1d6e5c1d38b7062bc3d2b8b"
  },
  "kernelspec": {
   "display_name": "Python 3.12.3 ('oracle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
